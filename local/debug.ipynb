{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "56dUiW4ArtQx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from datetime import (\n",
        "    datetime,\n",
        "    timedelta,\n",
        "    timezone\n",
        ")\n",
        "import pandas as pd\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ETZTpZ12rtQy"
      },
      "outputs": [],
      "source": [
        "def get_next_weekend():\n",
        "    today = datetime.now()\n",
        "    # Calculate days until next Saturday (5) and Sunday (6) respectively\n",
        "    days_until_friday = (4 - today.weekday()) % 7\n",
        "    days_until_saturday = (5 - today.weekday()) % 7\n",
        "    days_until_sunday = (6 - today.weekday()) % 7\n",
        "    # Calculate and format the dates\n",
        "    next_friday = (today + timedelta(days=days_until_friday)).strftime(\"%Y-%m-%d\")\n",
        "    next_saturday = (today + timedelta(days=days_until_saturday)).strftime(\"%Y-%m-%d\")\n",
        "    next_sunday = (today + timedelta(days=days_until_sunday)).strftime(\"%Y-%m-%d\")\n",
        "    # Return as a list\n",
        "    return [next_friday, next_saturday, next_sunday]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "I_VxNlwOrtQz"
      },
      "outputs": [],
      "source": [
        "def normalize_closeness_to_var(df, column_name, variable_to_compare):\n",
        "    # Compute closeness rank\n",
        "    df[f'closeness_rank'] = abs(df[column_name] - variable_to_compare)\n",
        "\n",
        "    # Find the maximum closeness rank\n",
        "    max_value = df[column_name].max()\n",
        "\n",
        "    # Handle the case where max_value is zero (all values are equal to variable_to_compare)\n",
        "    if max_value == 0:\n",
        "        df[f'normalized_closeness_{column_name}'] = 1  # All values get the max score\n",
        "    else:\n",
        "        # Normalize closeness\n",
        "        df[f'normalized_closeness_{column_name}'] = 1 - (df[f'closeness_rank'] / max_value)\n",
        "\n",
        "    # Drop the temporary column\n",
        "    df = df.drop('closeness_rank', axis=1)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ACCR_LrjrtQz",
        "outputId": "ba6d0df4-e9ec-48f5-9c48-fba11831543b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'./data'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def get_data_path():\n",
        "    current_folder = os.path.dirname(\".\")\n",
        "    return os.path.join(current_folder, \"./data\")\n",
        "\n",
        "def get_distances_path():\n",
        "    return os.path.join(get_data_path(), \"dist.csv\")\n",
        "\n",
        "get_data_path()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oHxCjtBQrtQ0"
      },
      "outputs": [],
      "source": [
        "# def surf_the_web(id, param_type):\n",
        "#     if param_type == \"wave\":\n",
        "#         url = f\"https://services.surfline.com/kbyg/spots/forecasts/wave?spotId={id}&days=5&intervalHours=1&cacheEnabled=true&units%5BswellHeight%5D=FT&units%5BwaveHeight%5D=FT\"\n",
        "#     elif param_type == \"wind\":\n",
        "#         url = f\"https://services.surfline.com/kbyg/spots/forecasts/wind?spotId={id}&days=5&intervalHours=1&corrected=false&cacheEnabled=true&units%5BwindSpeed%5D=MPH\"\n",
        "#     elif param_type == \"sunlight\":\n",
        "#         url = f\"https://services.surfline.com/kbyg/spots/forecasts/sunlight?spotId={id}&days=16&intervalHours=1\"\n",
        "#     elif param_type == \"rating\":\n",
        "#         url = f\"https://services.surfline.com/kbyg/spots/forecasts/rating?spotId={id}&days=5&intervalHours=1&cacheEnabled=true\"\n",
        "\n",
        "#     current_date = datetime.now().strftime(\"%Y%m%d\")\n",
        "\n",
        "#     file_name = os.path.join(get_data_path(), \"cache\", f\"{id}_{param_type}_{current_date}.json\")\n",
        "\n",
        "#     if os.path.exists(file_name):\n",
        "#         with open(file_name, 'r') as file:\n",
        "#             return json.loads(file.read())\n",
        "#     else:\n",
        "#         response = requests.get(url)\n",
        "\n",
        "#         # Check if the request was successful (status code 200)\n",
        "#         if response.status_code == 200:\n",
        "#             # Parse the JSON content\n",
        "#             json_object = response.json()\n",
        "\n",
        "#             json_string = json.dumps(json_object)\n",
        "\n",
        "#             with open(file_name, 'w') as file:\n",
        "#                 file.write(json_string)\n",
        "\n",
        "#             return json_object\n",
        "#         else:\n",
        "#             raise Exception(response)\n",
        "\n",
        "def surf_the_web(id, param_type):\n",
        "    if param_type == \"wave\":\n",
        "        url = f\"https://services.surfline.com/kbyg/spots/forecasts/wave?spotId={id}&days=5&intervalHours=1&cacheEnabled=true&units%5BswellHeight%5D=FT&units%5BwaveHeight%5D=FT\"\n",
        "    elif param_type == \"wind\":\n",
        "        url = f\"https://services.surfline.com/kbyg/spots/forecasts/wind?spotId={id}&days=5&intervalHours=1&corrected=false&cacheEnabled=true&units%5BwindSpeed%5D=MPH\"\n",
        "    elif param_type == \"sunlight\":\n",
        "        url = f\"https://services.surfline.com/kbyg/spots/forecasts/sunlight?spotId={id}&days=16&intervalHours=1\"\n",
        "    elif param_type == \"rating\":\n",
        "        url = f\"https://services.surfline.com/kbyg/spots/forecasts/rating?spotId={id}&days=5&intervalHours=1&cacheEnabled=true\"\n",
        "\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        json_object = response.json()\n",
        "        return json_object\n",
        "    else:\n",
        "        raise Exception(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZhWDyMWWrtQ0"
      },
      "outputs": [],
      "source": [
        "def unix_time_convert(unix):\n",
        "    return datetime.fromtimestamp(unix, tz=timezone.utc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2pCTbfZmrtQ1"
      },
      "outputs": [],
      "source": [
        "def extract(id):\n",
        "    wave_response = surf_the_web(id, 'wave')\n",
        "\n",
        "    wave_info = []  # Assuming this is defined somewhere above your code\n",
        "\n",
        "    for wave in wave_response[\"data\"][\"wave\"]:\n",
        "        spot_id = id\n",
        "        surf = wave.get(\"surf\", {})\n",
        "        swells = wave.get(\"swells\", [])\n",
        "        swell_period = -1\n",
        "\n",
        "        # Find the swell with the maximum impact if swells exist\n",
        "        if swells:\n",
        "            max_impact_swell = max(swells, key=lambda swell: swell.get(\"impact\", 0))\n",
        "            swell_period = max_impact_swell.get(\"period\", -1)\n",
        "\n",
        "        timestamp = unix_time_convert(wave.get(\"timestamp\", 0))\n",
        "\n",
        "        # Pull min and max wave sizes from the raw data\n",
        "        raw = surf.get(\"raw\", {})\n",
        "        min_wave_size = raw.get(\"min\", 0)  # Default to 0 if 'min' key is missing\n",
        "        max_wave_size = raw.get(\"max\", 0)  # Default to 0 if 'max' key is missing\n",
        "\n",
        "\n",
        "        wave_info_dict = {\n",
        "            'spot_id': spot_id,\n",
        "            'timestamp': timestamp,\n",
        "            'min_wave_size': min_wave_size,\n",
        "            'max_wave_size': max_wave_size,\n",
        "            'swell_period': swell_period\n",
        "        }\n",
        "        wave_info.append(wave_info_dict)\n",
        "\n",
        "\n",
        "    wind_response = surf_the_web(id, 'wind')\n",
        "    wind_info = []\n",
        "\n",
        "    for wind in wind_response[\"data\"][\"wind\"]:\n",
        "        spot_id = id\n",
        "        timestamp = unix_time_convert(wind[\"timestamp\"])\n",
        "        speed = wind[\"speed\"]\n",
        "        direction = wind[\"direction\"]\n",
        "        direction_type = wind[\"directionType\"]\n",
        "        wind_info_dict = {\n",
        "            'spot_id': spot_id,\n",
        "            'timestamp': timestamp,\n",
        "            'wind_speed': speed,\n",
        "            'wind_direction': direction,\n",
        "            'wind_type': direction_type\n",
        "        }\n",
        "        wind_info.append(wind_info_dict)\n",
        "\n",
        "    sun_response = surf_the_web(id, 'sunlight')\n",
        "    sun_info = []\n",
        "\n",
        "    for sun in sun_response[\"data\"][\"sunlight\"]:\n",
        "        spot_id = id\n",
        "        timestamp = unix_time_convert(sun[\"dawn\"])\n",
        "        date = timestamp.date()\n",
        "        dawn = unix_time_convert(sun[\"dawn\"])\n",
        "        sunrise = unix_time_convert(sun[\"sunrise\"])\n",
        "        sunset = unix_time_convert(sun[\"sunset\"])\n",
        "        dusk = unix_time_convert(sun[\"dusk\"])\n",
        "        sun_info_dict = {\n",
        "            'spot_id': spot_id,\n",
        "            'date': date,\n",
        "            'dawn': dawn,\n",
        "            'sunrise': sunrise,\n",
        "            'sunset': sunset,\n",
        "            'dusk': dusk\n",
        "        }\n",
        "        sun_info.append(sun_info_dict)\n",
        "\n",
        "    wave_df = pd.DataFrame(wave_info)\n",
        "    wind_df = pd.DataFrame(wind_info)\n",
        "    sun_df = pd.DataFrame(sun_info)\n",
        "\n",
        "    result_df = pd.merge(wave_df, wind_df, on=[\"spot_id\", \"timestamp\"], how=\"left\")\n",
        "    result_df['date'] = result_df[\"timestamp\"].dt.date\n",
        "    result_df = pd.merge(result_df, sun_df, on=[\"spot_id\", \"date\"], how=\"left\")\n",
        "    result_df = result_df.drop(columns=[\"date\"])\n",
        "\n",
        "\n",
        "    return result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "chj861RNrtQ1"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "# DIST_TRAVEL_LIMIT_HRS = 2\n",
        "TARGET_DATE = get_next_weekend()\n",
        "MIN_WAVE_SIZE = 2.5\n",
        "MAX_WAVE_SIZE = 3.5\n",
        "SWELL_PERIOD = 15\n",
        "IDEAL_DURATION = 1.25\n",
        "MAX_DURATION = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wfM4_rh0rtQ1"
      },
      "outputs": [],
      "source": [
        "# Fetch Surf Spot Data\n",
        "SURFLINE_URL = (\n",
        "    'https://services.surfline.com/kbyg/mapview'\n",
        "    '?south=48.90805939965008&west=-8.920898437500002&north=52.67638208083924&east=0.7580566406250001&'\n",
        ")\n",
        "\n",
        "response = requests.get(SURFLINE_URL)\n",
        "if response.status_code == 200:\n",
        "    resp_data = response.json()\n",
        "    spots = [\n",
        "        {\n",
        "            \"spot_id\": spot[\"_id\"],\n",
        "            \"spot_name\": spot[\"name\"],\n",
        "            \"sub_region\": spot[\"subregion\"][\"name\"],\n",
        "            \"lat\": spot[\"lat\"],\n",
        "            \"lon\": spot[\"lon\"],\n",
        "        }\n",
        "        for spot in resp_data[\"data\"][\"spots\"]\n",
        "    ]\n",
        "else:\n",
        "    raise RuntimeError(f\"Failed to fetch surf spot data: {response.status_code}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MA3MYuf4rtQ1"
      },
      "outputs": [],
      "source": [
        "# Convert spots to DataFrame\n",
        "spots_df = pd.DataFrame(spots)\n",
        "\n",
        "# Filter to Relevant Subregions\n",
        "INTEREST_SUBREGIONS = [\n",
        "    'Gower', 'North Cornwall', 'North Devon', 'Severn Estuary',\n",
        "    'South Devon', 'South Cornwall', 'South Pembrokeshire',\n",
        "    'Southern England West', 'Southern England East', 'West Cornwall',\n",
        "]\n",
        "spots_df = spots_df[spots_df[\"sub_region\"].isin(INTEREST_SUBREGIONS)]\n",
        "\n",
        "# Load Pre-calculated Distances\n",
        "distance_data = pd.read_csv(get_distances_path())\n",
        "distance_data['duration_hours'] = round(distance_data['duration_hours'], 1)\n",
        "\n",
        "# Filter distances based on MAX_DURATION\n",
        "filtered_distances_df = distance_data[distance_data[\"duration_hours\"] < MAX_DURATION]\n",
        "\n",
        "# Merge the DataFrames, keeping all columns from spots_df and only 'duration_hours' from distance_data\n",
        "filtered_spots_df = pd.merge(\n",
        "    spots_df,\n",
        "    filtered_distances_df[['spot_id', 'duration_hours']],  # Only include 'spot_id' and 'duration_hours'\n",
        "    on=\"spot_id\",  # Key for the join\n",
        "    how=\"inner\"    # Use 'inner' join to keep only matching rows\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "oD71O5NXrtQ2"
      },
      "outputs": [],
      "source": [
        "# Fetch Surf Data\n",
        "surf_data = pd.DataFrame()\n",
        "for spot_id in filtered_spots_df[\"spot_id\"]:\n",
        "    spot_info = extract(spot_id)\n",
        "    surf_data = pd.concat([surf_data, spot_info], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "d5yka2u6rtQ2"
      },
      "outputs": [],
      "source": [
        "# Merge and Filter by Time Range\n",
        "merged_data = pd.merge(filtered_spots_df, surf_data, on=\"spot_id\", how=\"left\")\n",
        "merged_data[\"timestamp\"] = pd.to_datetime(merged_data[\"timestamp\"])\n",
        "filtered_data = merged_data[\n",
        "    (merged_data[\"timestamp\"] >= merged_data[\"dawn\"])\n",
        "    & (merged_data[\"timestamp\"] <= merged_data[\"dusk\"])\n",
        "    & (merged_data[\"swell_period\"] > 0)\n",
        "    & (merged_data[\"min_wave_size\"] > 0)\n",
        "]\n",
        "\n",
        "# Filter by Target Date\n",
        "date_filtered_data = filtered_data[\n",
        "    filtered_data[\"timestamp\"].dt.strftime(\"%Y-%m-%d\").isin(TARGET_DATE)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEgDK5JvrtQ2",
        "outputId": "0a266cb3-0a2a-4bcd-fb23-3c9ae460ae06"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_59125/1232487106.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  date_filtered_data[\"wind_speed_bucket\"] = pd.cut(\n"
          ]
        }
      ],
      "source": [
        "# Bucket Wind Speeds\n",
        "BIN_EDGES = [0, 13, 16, 20, float(\"inf\")]\n",
        "BIN_LABELS = [\"0-12mph\", \"13-15mph\", \"16-20mph\", \"20+mph\"]\n",
        "date_filtered_data[\"wind_speed_bucket\"] = pd.cut(\n",
        "    date_filtered_data[\"wind_speed\"], bins=BIN_EDGES, labels=BIN_LABELS, right=False\n",
        ")\n",
        "\n",
        "# One-Hot Encode Categorical Features\n",
        "COLUMNS_TO_ENCODE = [\"wind_type\", \"wind_speed_bucket\"]\n",
        "encoded_df = pd.get_dummies(date_filtered_data[COLUMNS_TO_ENCODE], prefix=COLUMNS_TO_ENCODE)\n",
        "processed_data = pd.concat(\n",
        "    [date_filtered_data.drop(COLUMNS_TO_ENCODE, axis=1), encoded_df], axis=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "bVsm9u6trtQ2"
      },
      "outputs": [],
      "source": [
        "# Normalize Data\n",
        "processed_data = normalize_closeness_to_var(processed_data, \"swell_period\", SWELL_PERIOD)\n",
        "processed_data = normalize_closeness_to_var(processed_data, \"min_wave_size\", MIN_WAVE_SIZE)\n",
        "processed_data = normalize_closeness_to_var(processed_data, \"max_wave_size\", MAX_WAVE_SIZE)\n",
        "processed_data = normalize_closeness_to_var(processed_data, \"duration_hours\", IDEAL_DURATION)\n",
        "\n",
        "# Calculate Weighted Scores\n",
        "FEATURE_IMPORTANCE = {\n",
        "    \"wind_type_Cross-shore\": 3,\n",
        "    \"wind_type_Offshore\": 1,\n",
        "    \"wind_type_Onshore\": 8,\n",
        "    \"wind_speed_bucket_0-12mph\": 1,\n",
        "    \"wind_speed_bucket_13-15mph\": 1,\n",
        "    \"wind_speed_bucket_16-20mph\": 6,\n",
        "    \"wind_speed_bucket_20+mph\": 8,\n",
        "    \"normalized_values_swell_period\": 1,\n",
        "    \"normalized_closeness_min_wave_size\": 1,\n",
        "    \"normalized_closeness_max_wave_size\": 1,\n",
        "    \"normalized_values_duration_hours\": 3,\n",
        "}\n",
        "\n",
        "valid_columns = [col for col in FEATURE_IMPORTANCE if col in processed_data]\n",
        "processed_data[\"weighted_sum\"] = sum(\n",
        "    (6 - FEATURE_IMPORTANCE[col]) * processed_data[col] for col in valid_columns\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "n-9VN_1krtQ2"
      },
      "outputs": [],
      "source": [
        "# Assuming `processed_data` is your DataFrame\n",
        "# Convert `timestamp` column to datetime if not already done\n",
        "processed_data['timestamp'] = pd.to_datetime(processed_data['timestamp'])\n",
        "\n",
        "# Extract the date part\n",
        "processed_data['date'] = processed_data['timestamp'].dt.date\n",
        "\n",
        "# Aggregate `weighted_sum` by `date` and `spot_name` (location) by summing it\n",
        "aggregated_data = processed_data.groupby(['date', 'spot_name'], as_index=False)['weighted_sum'].sum()\n",
        "\n",
        "# Rank the spots based on the sum of `weighted_sum` within each date partition\n",
        "aggregated_data['rank'] = aggregated_data.groupby('date')['weighted_sum'].rank(ascending=False, method='dense')\n",
        "\n",
        "# Filter the top 3 spots for each date\n",
        "top_3_spots = aggregated_data[aggregated_data['rank'] < 5]\n",
        "\n",
        "# Merge with original data to get all columns for the top 3 spots\n",
        "\n",
        "extra_info = processed_data[[\n",
        "    \"date\", \n",
        "    \"spot_name\",\n",
        "    'sub_region',\n",
        "    'timestamp',\n",
        "    'duration_hours',\n",
        "    'min_wave_size',\n",
        "    'max_wave_size',\n",
        "    'swell_period',\n",
        "    'wind_speed',\n",
        "    'dawn',\n",
        "    'sunrise',\n",
        "    'sunset',\n",
        "    'dusk',\n",
        "    'wind_type_Cross-shore',\n",
        "    'wind_type_Offshore',\n",
        "    'wind_type_Onshore'\n",
        "]]\n",
        "\n",
        "top_3 = pd.merge(top_3_spots, extra_info, on=['date', 'spot_name'], how='left')\n",
        "\n",
        "# Sort the final output by date and rank\n",
        "top_3 = top_3.sort_values(by=['date', 'rank']).reset_index(drop=True)\n",
        "\n",
        "# Ensure missing wind type columns are added if not present\n",
        "if \"wind_type_Offshore\" not in top_3.columns:\n",
        "    top_3['wind_type_Offshore'] = False\n",
        "if \"wind_type_Onshore\" not in top_3.columns:\n",
        "    top_3['wind_type_Onshore'] = False\n",
        "if \"wind_type_Cross-shore\" not in top_3.columns:\n",
        "    top_3['wind_type_Cross-shore'] = False\n",
        "\n",
        "# Final column order\n",
        "final_columns = [\n",
        "    'spot_name',\n",
        "    'sub_region',\n",
        "    'timestamp',\n",
        "    'duration_hours',\n",
        "    'min_wave_size',\n",
        "    'max_wave_size',\n",
        "    'swell_period',\n",
        "    'wind_speed',\n",
        "    'dawn',\n",
        "    'sunrise',\n",
        "    'sunset',\n",
        "    'dusk',\n",
        "    'wind_type_Cross-shore',\n",
        "    'wind_type_Offshore',\n",
        "    'wind_type_Onshore',\n",
        "    'rank',\n",
        "    'weighted_sum'\n",
        "]\n",
        "\n",
        "# Ensure that all columns in `final_columns` exist in `top_3` before selecting them\n",
        "top_3 = top_3[final_columns]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>spot_name</th>\n",
              "      <th>sub_region</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>duration_hours</th>\n",
              "      <th>min_wave_size</th>\n",
              "      <th>max_wave_size</th>\n",
              "      <th>swell_period</th>\n",
              "      <th>wind_speed</th>\n",
              "      <th>dawn</th>\n",
              "      <th>sunrise</th>\n",
              "      <th>sunset</th>\n",
              "      <th>dusk</th>\n",
              "      <th>wind_type_Cross-shore</th>\n",
              "      <th>wind_type_Offshore</th>\n",
              "      <th>wind_type_Onshore</th>\n",
              "      <th>rank</th>\n",
              "      <th>weighted_sum</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Aberavon</td>\n",
              "      <td>Severn Estuary</td>\n",
              "      <td>2025-01-10 08:00:00+00:00</td>\n",
              "      <td>1.5</td>\n",
              "      <td>2.79769</td>\n",
              "      <td>4.00225</td>\n",
              "      <td>13</td>\n",
              "      <td>13.26281</td>\n",
              "      <td>2025-01-10 07:40:06+00:00</td>\n",
              "      <td>2025-01-10 08:19:19+00:00</td>\n",
              "      <td>2025-01-10 16:28:26+00:00</td>\n",
              "      <td>2025-01-10 17:07:38+00:00</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>1.0</td>\n",
              "      <td>195.261338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Aberavon</td>\n",
              "      <td>Severn Estuary</td>\n",
              "      <td>2025-01-10 09:00:00+00:00</td>\n",
              "      <td>1.5</td>\n",
              "      <td>2.60341</td>\n",
              "      <td>3.80797</td>\n",
              "      <td>13</td>\n",
              "      <td>12.61110</td>\n",
              "      <td>2025-01-10 07:40:06+00:00</td>\n",
              "      <td>2025-01-10 08:19:19+00:00</td>\n",
              "      <td>2025-01-10 16:28:26+00:00</td>\n",
              "      <td>2025-01-10 17:07:38+00:00</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>1.0</td>\n",
              "      <td>195.261338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Aberavon</td>\n",
              "      <td>Severn Estuary</td>\n",
              "      <td>2025-01-10 10:00:00+00:00</td>\n",
              "      <td>1.5</td>\n",
              "      <td>2.44798</td>\n",
              "      <td>3.65254</td>\n",
              "      <td>13</td>\n",
              "      <td>14.15506</td>\n",
              "      <td>2025-01-10 07:40:06+00:00</td>\n",
              "      <td>2025-01-10 08:19:19+00:00</td>\n",
              "      <td>2025-01-10 16:28:26+00:00</td>\n",
              "      <td>2025-01-10 17:07:38+00:00</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>1.0</td>\n",
              "      <td>195.261338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Aberavon</td>\n",
              "      <td>Severn Estuary</td>\n",
              "      <td>2025-01-10 11:00:00+00:00</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.98170</td>\n",
              "      <td>3.18626</td>\n",
              "      <td>12</td>\n",
              "      <td>15.36108</td>\n",
              "      <td>2025-01-10 07:40:06+00:00</td>\n",
              "      <td>2025-01-10 08:19:19+00:00</td>\n",
              "      <td>2025-01-10 16:28:26+00:00</td>\n",
              "      <td>2025-01-10 17:07:38+00:00</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>1.0</td>\n",
              "      <td>195.261338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Aberavon</td>\n",
              "      <td>Severn Estuary</td>\n",
              "      <td>2025-01-10 12:00:00+00:00</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.86513</td>\n",
              "      <td>3.06969</td>\n",
              "      <td>12</td>\n",
              "      <td>14.62094</td>\n",
              "      <td>2025-01-10 07:40:06+00:00</td>\n",
              "      <td>2025-01-10 08:19:19+00:00</td>\n",
              "      <td>2025-01-10 16:28:26+00:00</td>\n",
              "      <td>2025-01-10 17:07:38+00:00</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>1.0</td>\n",
              "      <td>195.261338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115</th>\n",
              "      <td>Rest Bay</td>\n",
              "      <td>Severn Estuary</td>\n",
              "      <td>2025-01-12 13:00:00+00:00</td>\n",
              "      <td>1.5</td>\n",
              "      <td>2.19301</td>\n",
              "      <td>3.63947</td>\n",
              "      <td>13</td>\n",
              "      <td>7.36298</td>\n",
              "      <td>2025-01-12 07:38:22+00:00</td>\n",
              "      <td>2025-01-12 08:17:14+00:00</td>\n",
              "      <td>2025-01-12 16:31:24+00:00</td>\n",
              "      <td>2025-01-12 17:10:15+00:00</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>4.0</td>\n",
              "      <td>188.816488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>Rest Bay</td>\n",
              "      <td>Severn Estuary</td>\n",
              "      <td>2025-01-12 14:00:00+00:00</td>\n",
              "      <td>1.5</td>\n",
              "      <td>2.14635</td>\n",
              "      <td>3.54615</td>\n",
              "      <td>13</td>\n",
              "      <td>5.71363</td>\n",
              "      <td>2025-01-12 07:38:22+00:00</td>\n",
              "      <td>2025-01-12 08:17:14+00:00</td>\n",
              "      <td>2025-01-12 16:31:24+00:00</td>\n",
              "      <td>2025-01-12 17:10:15+00:00</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>4.0</td>\n",
              "      <td>188.816488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>Rest Bay</td>\n",
              "      <td>Severn Estuary</td>\n",
              "      <td>2025-01-12 15:00:00+00:00</td>\n",
              "      <td>1.5</td>\n",
              "      <td>2.05303</td>\n",
              "      <td>3.49949</td>\n",
              "      <td>12</td>\n",
              "      <td>4.00314</td>\n",
              "      <td>2025-01-12 07:38:22+00:00</td>\n",
              "      <td>2025-01-12 08:17:14+00:00</td>\n",
              "      <td>2025-01-12 16:31:24+00:00</td>\n",
              "      <td>2025-01-12 17:10:15+00:00</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>4.0</td>\n",
              "      <td>188.816488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>Rest Bay</td>\n",
              "      <td>Severn Estuary</td>\n",
              "      <td>2025-01-12 16:00:00+00:00</td>\n",
              "      <td>1.5</td>\n",
              "      <td>2.00637</td>\n",
              "      <td>3.40617</td>\n",
              "      <td>12</td>\n",
              "      <td>4.19779</td>\n",
              "      <td>2025-01-12 07:38:22+00:00</td>\n",
              "      <td>2025-01-12 08:17:14+00:00</td>\n",
              "      <td>2025-01-12 16:31:24+00:00</td>\n",
              "      <td>2025-01-12 17:10:15+00:00</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>4.0</td>\n",
              "      <td>188.816488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119</th>\n",
              "      <td>Rest Bay</td>\n",
              "      <td>Severn Estuary</td>\n",
              "      <td>2025-01-12 17:00:00+00:00</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.91305</td>\n",
              "      <td>3.35951</td>\n",
              "      <td>12</td>\n",
              "      <td>2.93567</td>\n",
              "      <td>2025-01-12 07:38:22+00:00</td>\n",
              "      <td>2025-01-12 08:17:14+00:00</td>\n",
              "      <td>2025-01-12 16:31:24+00:00</td>\n",
              "      <td>2025-01-12 17:10:15+00:00</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>4.0</td>\n",
              "      <td>188.816488</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>120 rows × 17 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    spot_name      sub_region                 timestamp  duration_hours  \\\n",
              "0    Aberavon  Severn Estuary 2025-01-10 08:00:00+00:00             1.5   \n",
              "1    Aberavon  Severn Estuary 2025-01-10 09:00:00+00:00             1.5   \n",
              "2    Aberavon  Severn Estuary 2025-01-10 10:00:00+00:00             1.5   \n",
              "3    Aberavon  Severn Estuary 2025-01-10 11:00:00+00:00             1.5   \n",
              "4    Aberavon  Severn Estuary 2025-01-10 12:00:00+00:00             1.5   \n",
              "..        ...             ...                       ...             ...   \n",
              "115  Rest Bay  Severn Estuary 2025-01-12 13:00:00+00:00             1.5   \n",
              "116  Rest Bay  Severn Estuary 2025-01-12 14:00:00+00:00             1.5   \n",
              "117  Rest Bay  Severn Estuary 2025-01-12 15:00:00+00:00             1.5   \n",
              "118  Rest Bay  Severn Estuary 2025-01-12 16:00:00+00:00             1.5   \n",
              "119  Rest Bay  Severn Estuary 2025-01-12 17:00:00+00:00             1.5   \n",
              "\n",
              "     min_wave_size  max_wave_size  swell_period  wind_speed  \\\n",
              "0          2.79769        4.00225            13    13.26281   \n",
              "1          2.60341        3.80797            13    12.61110   \n",
              "2          2.44798        3.65254            13    14.15506   \n",
              "3          1.98170        3.18626            12    15.36108   \n",
              "4          1.86513        3.06969            12    14.62094   \n",
              "..             ...            ...           ...         ...   \n",
              "115        2.19301        3.63947            13     7.36298   \n",
              "116        2.14635        3.54615            13     5.71363   \n",
              "117        2.05303        3.49949            12     4.00314   \n",
              "118        2.00637        3.40617            12     4.19779   \n",
              "119        1.91305        3.35951            12     2.93567   \n",
              "\n",
              "                         dawn                   sunrise  \\\n",
              "0   2025-01-10 07:40:06+00:00 2025-01-10 08:19:19+00:00   \n",
              "1   2025-01-10 07:40:06+00:00 2025-01-10 08:19:19+00:00   \n",
              "2   2025-01-10 07:40:06+00:00 2025-01-10 08:19:19+00:00   \n",
              "3   2025-01-10 07:40:06+00:00 2025-01-10 08:19:19+00:00   \n",
              "4   2025-01-10 07:40:06+00:00 2025-01-10 08:19:19+00:00   \n",
              "..                        ...                       ...   \n",
              "115 2025-01-12 07:38:22+00:00 2025-01-12 08:17:14+00:00   \n",
              "116 2025-01-12 07:38:22+00:00 2025-01-12 08:17:14+00:00   \n",
              "117 2025-01-12 07:38:22+00:00 2025-01-12 08:17:14+00:00   \n",
              "118 2025-01-12 07:38:22+00:00 2025-01-12 08:17:14+00:00   \n",
              "119 2025-01-12 07:38:22+00:00 2025-01-12 08:17:14+00:00   \n",
              "\n",
              "                       sunset                      dusk  \\\n",
              "0   2025-01-10 16:28:26+00:00 2025-01-10 17:07:38+00:00   \n",
              "1   2025-01-10 16:28:26+00:00 2025-01-10 17:07:38+00:00   \n",
              "2   2025-01-10 16:28:26+00:00 2025-01-10 17:07:38+00:00   \n",
              "3   2025-01-10 16:28:26+00:00 2025-01-10 17:07:38+00:00   \n",
              "4   2025-01-10 16:28:26+00:00 2025-01-10 17:07:38+00:00   \n",
              "..                        ...                       ...   \n",
              "115 2025-01-12 16:31:24+00:00 2025-01-12 17:10:15+00:00   \n",
              "116 2025-01-12 16:31:24+00:00 2025-01-12 17:10:15+00:00   \n",
              "117 2025-01-12 16:31:24+00:00 2025-01-12 17:10:15+00:00   \n",
              "118 2025-01-12 16:31:24+00:00 2025-01-12 17:10:15+00:00   \n",
              "119 2025-01-12 16:31:24+00:00 2025-01-12 17:10:15+00:00   \n",
              "\n",
              "     wind_type_Cross-shore  wind_type_Offshore  wind_type_Onshore  rank  \\\n",
              "0                    False                True              False   1.0   \n",
              "1                    False                True              False   1.0   \n",
              "2                    False                True              False   1.0   \n",
              "3                    False                True              False   1.0   \n",
              "4                    False                True              False   1.0   \n",
              "..                     ...                 ...                ...   ...   \n",
              "115                   True               False              False   4.0   \n",
              "116                   True               False              False   4.0   \n",
              "117                   True               False              False   4.0   \n",
              "118                  False                True              False   4.0   \n",
              "119                  False                True              False   4.0   \n",
              "\n",
              "     weighted_sum  \n",
              "0      195.261338  \n",
              "1      195.261338  \n",
              "2      195.261338  \n",
              "3      195.261338  \n",
              "4      195.261338  \n",
              "..            ...  \n",
              "115    188.816488  \n",
              "116    188.816488  \n",
              "117    188.816488  \n",
              "118    188.816488  \n",
              "119    188.816488  \n",
              "\n",
              "[120 rows x 17 columns]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "top_3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
