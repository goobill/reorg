{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "56dUiW4ArtQx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from datetime import (\n",
        "    datetime,\n",
        "    timedelta,\n",
        "    timezone\n",
        ")\n",
        "import pandas as pd\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ETZTpZ12rtQy"
      },
      "outputs": [],
      "source": [
        "def get_next_weekend():\n",
        "    today = datetime.now()\n",
        "    # Calculate days until next Saturday (5) and Sunday (6) respectively\n",
        "    days_until_friday = (4 - today.weekday()) % 7\n",
        "    days_until_saturday = (5 - today.weekday()) % 7\n",
        "    days_until_sunday = (6 - today.weekday()) % 7\n",
        "    # Calculate and format the dates\n",
        "    next_friday = (today + timedelta(days=days_until_friday)).strftime(\"%Y-%m-%d\")\n",
        "    next_saturday = (today + timedelta(days=days_until_saturday)).strftime(\"%Y-%m-%d\")\n",
        "    next_sunday = (today + timedelta(days=days_until_sunday)).strftime(\"%Y-%m-%d\")\n",
        "    # Return as a list\n",
        "    return [next_friday, next_saturday, next_sunday]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "I_VxNlwOrtQz"
      },
      "outputs": [],
      "source": [
        "def normalize_closeness_to_var(df, column_name, variable_to_compare):\n",
        "    # Compute closeness rank\n",
        "    df[f'closeness_rank'] = abs(df[column_name] - variable_to_compare)\n",
        "\n",
        "    # Find the maximum closeness rank\n",
        "    max_value = df[column_name].max()\n",
        "\n",
        "    # Handle the case where max_value is zero (all values are equal to variable_to_compare)\n",
        "    if max_value == 0:\n",
        "        df[f'normalized_closeness_{column_name}'] = 1  # All values get the max score\n",
        "    else:\n",
        "        # Normalize closeness\n",
        "        df[f'normalized_closeness_{column_name}'] = 1 - (df[f'closeness_rank'] / max_value)\n",
        "\n",
        "    # Drop the temporary column\n",
        "    df = df.drop('closeness_rank', axis=1)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ACCR_LrjrtQz",
        "outputId": "ba6d0df4-e9ec-48f5-9c48-fba11831543b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'./data'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def get_data_path():\n",
        "    current_folder = os.path.dirname(\".\")\n",
        "    return os.path.join(current_folder, \"./data\")\n",
        "\n",
        "def get_distances_path():\n",
        "    return os.path.join(get_data_path(), \"dist.csv\")\n",
        "\n",
        "get_data_path()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oHxCjtBQrtQ0"
      },
      "outputs": [],
      "source": [
        "# def surf_the_web(id, param_type):\n",
        "#     if param_type == \"wave\":\n",
        "#         url = f\"https://services.surfline.com/kbyg/spots/forecasts/wave?spotId={id}&days=5&intervalHours=1&cacheEnabled=true&units%5BswellHeight%5D=FT&units%5BwaveHeight%5D=FT\"\n",
        "#     elif param_type == \"wind\":\n",
        "#         url = f\"https://services.surfline.com/kbyg/spots/forecasts/wind?spotId={id}&days=5&intervalHours=1&corrected=false&cacheEnabled=true&units%5BwindSpeed%5D=MPH\"\n",
        "#     elif param_type == \"sunlight\":\n",
        "#         url = f\"https://services.surfline.com/kbyg/spots/forecasts/sunlight?spotId={id}&days=16&intervalHours=1\"\n",
        "#     elif param_type == \"rating\":\n",
        "#         url = f\"https://services.surfline.com/kbyg/spots/forecasts/rating?spotId={id}&days=5&intervalHours=1&cacheEnabled=true\"\n",
        "\n",
        "#     current_date = datetime.now().strftime(\"%Y%m%d\")\n",
        "\n",
        "#     file_name = os.path.join(get_data_path(), \"cache\", f\"{id}_{param_type}_{current_date}.json\")\n",
        "\n",
        "#     if os.path.exists(file_name):\n",
        "#         with open(file_name, 'r') as file:\n",
        "#             return json.loads(file.read())\n",
        "#     else:\n",
        "#         response = requests.get(url)\n",
        "\n",
        "#         # Check if the request was successful (status code 200)\n",
        "#         if response.status_code == 200:\n",
        "#             # Parse the JSON content\n",
        "#             json_object = response.json()\n",
        "\n",
        "#             json_string = json.dumps(json_object)\n",
        "\n",
        "#             with open(file_name, 'w') as file:\n",
        "#                 file.write(json_string)\n",
        "\n",
        "#             return json_object\n",
        "#         else:\n",
        "#             raise Exception(response)\n",
        "\n",
        "def surf_the_web(id, param_type):\n",
        "    if param_type == \"wave\":\n",
        "        url = f\"https://services.surfline.com/kbyg/spots/forecasts/wave?spotId={id}&days=5&intervalHours=1&cacheEnabled=true&units%5BswellHeight%5D=FT&units%5BwaveHeight%5D=FT\"\n",
        "    elif param_type == \"wind\":\n",
        "        url = f\"https://services.surfline.com/kbyg/spots/forecasts/wind?spotId={id}&days=5&intervalHours=1&corrected=false&cacheEnabled=true&units%5BwindSpeed%5D=MPH\"\n",
        "    elif param_type == \"sunlight\":\n",
        "        url = f\"https://services.surfline.com/kbyg/spots/forecasts/sunlight?spotId={id}&days=16&intervalHours=1\"\n",
        "    elif param_type == \"rating\":\n",
        "        url = f\"https://services.surfline.com/kbyg/spots/forecasts/rating?spotId={id}&days=5&intervalHours=1&cacheEnabled=true\"\n",
        "\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        json_object = response.json()\n",
        "        return json_object\n",
        "    else:\n",
        "        raise Exception(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZhWDyMWWrtQ0"
      },
      "outputs": [],
      "source": [
        "def unix_time_convert(unix):\n",
        "    return datetime.fromtimestamp(unix, tz=timezone.utc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2pCTbfZmrtQ1"
      },
      "outputs": [],
      "source": [
        "def extract(id):\n",
        "    wave_response = surf_the_web(id, 'wave')\n",
        "\n",
        "    wave_info = []  # Assuming this is defined somewhere above your code\n",
        "\n",
        "    for wave in wave_response[\"data\"][\"wave\"]:\n",
        "        spot_id = id\n",
        "        surf = wave.get(\"surf\", {})\n",
        "        swells = wave.get(\"swells\", [])\n",
        "        swell_period = -1\n",
        "\n",
        "        # Find the swell with the maximum impact if swells exist\n",
        "        if swells:\n",
        "            max_impact_swell = max(swells, key=lambda swell: swell.get(\"impact\", 0))\n",
        "            swell_period = max_impact_swell.get(\"period\", -1)\n",
        "\n",
        "        timestamp = unix_time_convert(wave.get(\"timestamp\", 0))\n",
        "\n",
        "        # Pull min and max wave sizes from the raw data\n",
        "        raw = surf.get(\"raw\", {})\n",
        "        min_wave_size = raw.get(\"min\", 0)  # Default to 0 if 'min' key is missing\n",
        "        max_wave_size = raw.get(\"max\", 0)  # Default to 0 if 'max' key is missing\n",
        "\n",
        "\n",
        "        wave_info_dict = {\n",
        "            'spot_id': spot_id,\n",
        "            'timestamp': timestamp,\n",
        "            'min_wave_size': min_wave_size,\n",
        "            'max_wave_size': max_wave_size,\n",
        "            'swell_period': swell_period\n",
        "        }\n",
        "        wave_info.append(wave_info_dict)\n",
        "\n",
        "\n",
        "    wind_response = surf_the_web(id, 'wind')\n",
        "    wind_info = []\n",
        "\n",
        "    for wind in wind_response[\"data\"][\"wind\"]:\n",
        "        spot_id = id\n",
        "        timestamp = unix_time_convert(wind[\"timestamp\"])\n",
        "        speed = wind[\"speed\"]\n",
        "        direction = wind[\"direction\"]\n",
        "        direction_type = wind[\"directionType\"]\n",
        "        wind_info_dict = {\n",
        "            'spot_id': spot_id,\n",
        "            'timestamp': timestamp,\n",
        "            'wind_speed': speed,\n",
        "            'wind_direction': direction,\n",
        "            'wind_type': direction_type\n",
        "        }\n",
        "        wind_info.append(wind_info_dict)\n",
        "\n",
        "    sun_response = surf_the_web(id, 'sunlight')\n",
        "    sun_info = []\n",
        "\n",
        "    for sun in sun_response[\"data\"][\"sunlight\"]:\n",
        "        spot_id = id\n",
        "        timestamp = unix_time_convert(sun[\"dawn\"])\n",
        "        date = timestamp.date()\n",
        "        dawn = unix_time_convert(sun[\"dawn\"])\n",
        "        sunrise = unix_time_convert(sun[\"sunrise\"])\n",
        "        sunset = unix_time_convert(sun[\"sunset\"])\n",
        "        dusk = unix_time_convert(sun[\"dusk\"])\n",
        "        sun_info_dict = {\n",
        "            'spot_id': spot_id,\n",
        "            'date': date,\n",
        "            'dawn': dawn,\n",
        "            'sunrise': sunrise,\n",
        "            'sunset': sunset,\n",
        "            'dusk': dusk\n",
        "        }\n",
        "        sun_info.append(sun_info_dict)\n",
        "\n",
        "    wave_df = pd.DataFrame(wave_info)\n",
        "    wind_df = pd.DataFrame(wind_info)\n",
        "    sun_df = pd.DataFrame(sun_info)\n",
        "\n",
        "    result_df = pd.merge(wave_df, wind_df, on=[\"spot_id\", \"timestamp\"], how=\"left\")\n",
        "    result_df['date'] = result_df[\"timestamp\"].dt.date\n",
        "    result_df = pd.merge(result_df, sun_df, on=[\"spot_id\", \"date\"], how=\"left\")\n",
        "    result_df = result_df.drop(columns=[\"date\"])\n",
        "\n",
        "\n",
        "    return result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "chj861RNrtQ1"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "# DIST_TRAVEL_LIMIT_HRS = 2\n",
        "TARGET_DATE = get_next_weekend()\n",
        "MIN_WAVE_SIZE = 2.5\n",
        "MAX_WAVE_SIZE = 3.5\n",
        "SWELL_PERIOD = 15\n",
        "IDEAL_DURATION = 1.25\n",
        "MAX_DURATION = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wfM4_rh0rtQ1"
      },
      "outputs": [],
      "source": [
        "# Fetch Surf Spot Data\n",
        "SURFLINE_URL = (\n",
        "    'https://services.surfline.com/kbyg/mapview'\n",
        "    '?south=48.90805939965008&west=-8.920898437500002&north=52.67638208083924&east=0.7580566406250001&'\n",
        ")\n",
        "\n",
        "response = requests.get(SURFLINE_URL)\n",
        "if response.status_code == 200:\n",
        "    resp_data = response.json()\n",
        "    spots = [\n",
        "        {\n",
        "            \"spot_id\": spot[\"_id\"],\n",
        "            \"spot_name\": spot[\"name\"],\n",
        "            \"sub_region\": spot[\"subregion\"][\"name\"],\n",
        "            \"lat\": spot[\"lat\"],\n",
        "            \"lon\": spot[\"lon\"],\n",
        "        }\n",
        "        for spot in resp_data[\"data\"][\"spots\"]\n",
        "    ]\n",
        "else:\n",
        "    raise RuntimeError(f\"Failed to fetch surf spot data: {response.status_code}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MA3MYuf4rtQ1"
      },
      "outputs": [],
      "source": [
        "# Convert spots to DataFrame\n",
        "spots_df = pd.DataFrame(spots)\n",
        "\n",
        "# Filter to Relevant Subregions\n",
        "INTEREST_SUBREGIONS = [\n",
        "    'Gower', 'North Cornwall', 'North Devon', 'Severn Estuary',\n",
        "    'South Devon', 'South Cornwall', 'South Pembrokeshire',\n",
        "    'Southern England West', 'Southern England East', 'West Cornwall',\n",
        "]\n",
        "spots_df = spots_df[spots_df[\"sub_region\"].isin(INTEREST_SUBREGIONS)]\n",
        "\n",
        "# Load Pre-calculated Distances\n",
        "distance_data = pd.read_csv(get_distances_path())\n",
        "distance_data['duration_hours'] = round(distance_data['duration_hours'], 1)\n",
        "\n",
        "# Filter distances based on MAX_DURATION\n",
        "filtered_distances_df = distance_data[distance_data[\"duration_hours\"] < MAX_DURATION]\n",
        "\n",
        "# Merge the DataFrames, keeping all columns from spots_df and only 'duration_hours' from distance_data\n",
        "filtered_spots_df = pd.merge(\n",
        "    spots_df,\n",
        "    filtered_distances_df[['spot_id', 'duration_hours']],  # Only include 'spot_id' and 'duration_hours'\n",
        "    on=\"spot_id\",  # Key for the join\n",
        "    how=\"inner\"    # Use 'inner' join to keep only matching rows\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "oD71O5NXrtQ2"
      },
      "outputs": [],
      "source": [
        "# Fetch Surf Data\n",
        "surf_data = pd.DataFrame()\n",
        "for spot_id in filtered_spots_df[\"spot_id\"]:\n",
        "    spot_info = extract(spot_id)\n",
        "    surf_data = pd.concat([surf_data, spot_info], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "d5yka2u6rtQ2"
      },
      "outputs": [],
      "source": [
        "# Merge and Filter by Time Range\n",
        "merged_data = pd.merge(filtered_spots_df, surf_data, on=\"spot_id\", how=\"left\")\n",
        "merged_data[\"timestamp\"] = pd.to_datetime(merged_data[\"timestamp\"])\n",
        "filtered_data = merged_data[\n",
        "    (merged_data[\"timestamp\"] >= merged_data[\"dawn\"])\n",
        "    & (merged_data[\"timestamp\"] <= merged_data[\"dusk\"])\n",
        "    & (merged_data[\"swell_period\"] > 0)\n",
        "    & (merged_data[\"min_wave_size\"] > 0)\n",
        "]\n",
        "\n",
        "# Filter by Target Date\n",
        "date_filtered_data = filtered_data[\n",
        "    filtered_data[\"timestamp\"].dt.strftime(\"%Y-%m-%d\").isin(TARGET_DATE)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEgDK5JvrtQ2",
        "outputId": "0a266cb3-0a2a-4bcd-fb23-3c9ae460ae06"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_80558/1232487106.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  date_filtered_data[\"wind_speed_bucket\"] = pd.cut(\n"
          ]
        }
      ],
      "source": [
        "# Bucket Wind Speeds\n",
        "BIN_EDGES = [0, 13, 16, 20, float(\"inf\")]\n",
        "BIN_LABELS = [\"0-12mph\", \"13-15mph\", \"16-20mph\", \"20+mph\"]\n",
        "date_filtered_data[\"wind_speed_bucket\"] = pd.cut(\n",
        "    date_filtered_data[\"wind_speed\"], bins=BIN_EDGES, labels=BIN_LABELS, right=False\n",
        ")\n",
        "\n",
        "# One-Hot Encode Categorical Features\n",
        "COLUMNS_TO_ENCODE = [\"wind_type\", \"wind_speed_bucket\"]\n",
        "encoded_df = pd.get_dummies(date_filtered_data[COLUMNS_TO_ENCODE], prefix=COLUMNS_TO_ENCODE)\n",
        "processed_data = pd.concat(\n",
        "    [date_filtered_data.drop(COLUMNS_TO_ENCODE, axis=1), encoded_df], axis=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "bVsm9u6trtQ2"
      },
      "outputs": [],
      "source": [
        "# Normalize Data\n",
        "processed_data = normalize_closeness_to_var(processed_data, \"swell_period\", SWELL_PERIOD)\n",
        "processed_data = normalize_closeness_to_var(processed_data, \"min_wave_size\", MIN_WAVE_SIZE)\n",
        "processed_data = normalize_closeness_to_var(processed_data, \"max_wave_size\", MAX_WAVE_SIZE)\n",
        "processed_data = normalize_closeness_to_var(processed_data, \"duration_hours\", IDEAL_DURATION)\n",
        "\n",
        "# Calculate Weighted Scores\n",
        "FEATURE_IMPORTANCE = {\n",
        "    \"wind_type_Cross-shore\": 3,\n",
        "    \"wind_type_Offshore\": 1,\n",
        "    \"wind_type_Onshore\": 8,\n",
        "    \"wind_speed_bucket_0-12mph\": 1,\n",
        "    \"wind_speed_bucket_13-15mph\": 1,\n",
        "    \"wind_speed_bucket_16-20mph\": 6,\n",
        "    \"wind_speed_bucket_20+mph\": 8,\n",
        "    \"normalized_values_swell_period\": 1,\n",
        "    \"normalized_closeness_min_wave_size\": 1,\n",
        "    \"normalized_closeness_max_wave_size\": 1,\n",
        "    \"normalized_values_duration_hours\": 3,\n",
        "}\n",
        "\n",
        "valid_columns = [col for col in FEATURE_IMPORTANCE if col in processed_data]\n",
        "processed_data[\"weighted_sum\"] = sum(\n",
        "    (6 - FEATURE_IMPORTANCE[col]) * processed_data[col] for col in valid_columns\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "n-9VN_1krtQ2"
      },
      "outputs": [],
      "source": [
        "# Assuming `processed_data` is your DataFrame\n",
        "# Convert `timestamp` column to datetime if not already done\n",
        "processed_data['timestamp'] = pd.to_datetime(processed_data['timestamp'])\n",
        "\n",
        "# Extract the date part\n",
        "processed_data['date'] = processed_data['timestamp'].dt.date\n",
        "\n",
        "# Aggregate `weighted_sum` by `date` and `spot_name` (location) by summing it\n",
        "aggregated_data = processed_data.groupby(['date', 'spot_name'], as_index=False)['weighted_sum'].sum()\n",
        "\n",
        "# Rank the spots based on the sum of `weighted_sum` within each date partition\n",
        "aggregated_data['rank'] = aggregated_data.groupby('date')['weighted_sum'].rank(ascending=False, method='dense')\n",
        "\n",
        "aggregated_data['total_weighted_sum'] = aggregated_data['weighted_sum']\n",
        "aggregated_data = aggregated_data.drop('weighted_sum', axis=1)\n",
        "\n",
        "# Filter the top 3 spots for each date\n",
        "top_3_spots = aggregated_data[aggregated_data['rank'] < 5]\n",
        "\n",
        "# Merge with original data to get all columns for the top 3 spots\n",
        "\n",
        "extra_info = processed_data[[\n",
        "    \"date\", \n",
        "    \"spot_name\",\n",
        "    'sub_region',\n",
        "    'timestamp',\n",
        "    'duration_hours',\n",
        "    'min_wave_size',\n",
        "    'max_wave_size',\n",
        "    'swell_period',\n",
        "    'wind_speed',\n",
        "    'dawn',\n",
        "    'sunrise',\n",
        "    'sunset',\n",
        "    'dusk',\n",
        "    'wind_type_Cross-shore',\n",
        "    'wind_type_Offshore',\n",
        "    'wind_type_Onshore',\n",
        "    'weighted_sum'\n",
        "]]\n",
        "\n",
        "top_3 = pd.merge(top_3_spots, extra_info, on=['date', 'spot_name'], how='left')\n",
        "\n",
        "# Sort the final output by date and rank\n",
        "top_3 = top_3.sort_values(by=['date', 'rank']).reset_index(drop=True)\n",
        "\n",
        "# Ensure missing wind type columns are added if not present\n",
        "if \"wind_type_Offshore\" not in top_3.columns:\n",
        "    top_3['wind_type_Offshore'] = False\n",
        "if \"wind_type_Onshore\" not in top_3.columns:\n",
        "    top_3['wind_type_Onshore'] = False\n",
        "if \"wind_type_Cross-shore\" not in top_3.columns:\n",
        "    top_3['wind_type_Cross-shore'] = False\n",
        "\n",
        "# Final column order\n",
        "final_columns = [\n",
        "    'spot_name',\n",
        "    'sub_region',\n",
        "    'timestamp',\n",
        "    'duration_hours',\n",
        "    'min_wave_size',\n",
        "    'max_wave_size',\n",
        "    'swell_period',\n",
        "    'wind_speed',\n",
        "    'dawn',\n",
        "    'sunrise',\n",
        "    'sunset',\n",
        "    'dusk',\n",
        "    'wind_type_Cross-shore',\n",
        "    'wind_type_Offshore',\n",
        "    'wind_type_Onshore',\n",
        "    'rank',\n",
        "    'weighted_sum',\n",
        "    'total_weighted_sum'\n",
        "]\n",
        "\n",
        "# Ensure that all columns in `final_columns` exist in `top_3` before selecting them\n",
        "top_3 = top_3[final_columns]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_80558/2369513599.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  f['date'] = f['timestamp'].dt.date\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>spot_name</th>\n",
              "      <th>weighted_sum</th>\n",
              "      <th>total_weighted_sum</th>\n",
              "      <th>rank</th>\n",
              "      <th>date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Aberavon</td>\n",
              "      <td>19.618773</td>\n",
              "      <td>195.261338</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2025-01-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Aberavon</td>\n",
              "      <td>19.807544</td>\n",
              "      <td>195.261338</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2025-01-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Aberavon</td>\n",
              "      <td>19.904257</td>\n",
              "      <td>195.261338</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2025-01-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Aberavon</td>\n",
              "      <td>19.588377</td>\n",
              "      <td>195.261338</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2025-01-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Aberavon</td>\n",
              "      <td>19.475113</td>\n",
              "      <td>195.261338</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2025-01-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115</th>\n",
              "      <td>Rest Bay</td>\n",
              "      <td>17.777040</td>\n",
              "      <td>188.816488</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2025-01-12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>Rest Bay</td>\n",
              "      <td>17.794644</td>\n",
              "      <td>188.816488</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2025-01-12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>Rest Bay</td>\n",
              "      <td>17.766453</td>\n",
              "      <td>188.816488</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2025-01-12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>Rest Bay</td>\n",
              "      <td>19.700136</td>\n",
              "      <td>188.816488</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2025-01-12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119</th>\n",
              "      <td>Rest Bay</td>\n",
              "      <td>19.630443</td>\n",
              "      <td>188.816488</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2025-01-12</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>110 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    spot_name  weighted_sum  total_weighted_sum  rank        date\n",
              "0    Aberavon     19.618773          195.261338   1.0  2025-01-10\n",
              "1    Aberavon     19.807544          195.261338   1.0  2025-01-10\n",
              "2    Aberavon     19.904257          195.261338   1.0  2025-01-10\n",
              "3    Aberavon     19.588377          195.261338   1.0  2025-01-10\n",
              "4    Aberavon     19.475113          195.261338   1.0  2025-01-10\n",
              "..        ...           ...                 ...   ...         ...\n",
              "115  Rest Bay     17.777040          188.816488   4.0  2025-01-12\n",
              "116  Rest Bay     17.794644          188.816488   4.0  2025-01-12\n",
              "117  Rest Bay     17.766453          188.816488   4.0  2025-01-12\n",
              "118  Rest Bay     19.700136          188.816488   4.0  2025-01-12\n",
              "119  Rest Bay     19.630443          188.816488   4.0  2025-01-12\n",
              "\n",
              "[110 rows x 5 columns]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "f = top_3[[\"spot_name\", \"weighted_sum\", \"total_weighted_sum\", \"rank\", \"timestamp\"]]\n",
        "\n",
        "f['date'] = f['timestamp'].dt.date\n",
        "\n",
        "f[[\"spot_name\", \"weighted_sum\", \"total_weighted_sum\", \"rank\", \"date\"]].drop_duplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>spot_name</th>\n",
              "      <th>weighted_sum</th>\n",
              "      <th>rank</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2025-01-10</td>\n",
              "      <td>Aberavon</td>\n",
              "      <td>195.261338</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2025-01-10</td>\n",
              "      <td>Avon Beach</td>\n",
              "      <td>121.618158</td>\n",
              "      <td>68.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2025-01-10</td>\n",
              "      <td>Bantham</td>\n",
              "      <td>119.073191</td>\n",
              "      <td>71.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2025-01-10</td>\n",
              "      <td>Barton on Sea</td>\n",
              "      <td>191.745333</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2025-01-10</td>\n",
              "      <td>Beer Point</td>\n",
              "      <td>167.192283</td>\n",
              "      <td>36.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>312</th>\n",
              "      <td>2025-01-12</td>\n",
              "      <td>Westward Ho!</td>\n",
              "      <td>167.316558</td>\n",
              "      <td>20.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>313</th>\n",
              "      <td>2025-01-12</td>\n",
              "      <td>Whitsand Bay</td>\n",
              "      <td>129.146977</td>\n",
              "      <td>48.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>314</th>\n",
              "      <td>2025-01-12</td>\n",
              "      <td>Widemouth Bay</td>\n",
              "      <td>158.416761</td>\n",
              "      <td>26.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>315</th>\n",
              "      <td>2025-01-12</td>\n",
              "      <td>Wiseman's Bridge</td>\n",
              "      <td>111.968685</td>\n",
              "      <td>82.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>316</th>\n",
              "      <td>2025-01-12</td>\n",
              "      <td>Woolacombe Bay</td>\n",
              "      <td>171.225299</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>317 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           date         spot_name  weighted_sum  rank\n",
              "0    2025-01-10          Aberavon    195.261338   1.0\n",
              "1    2025-01-10        Avon Beach    121.618158  68.0\n",
              "2    2025-01-10           Bantham    119.073191  71.0\n",
              "3    2025-01-10     Barton on Sea    191.745333   8.0\n",
              "4    2025-01-10        Beer Point    167.192283  36.0\n",
              "..          ...               ...           ...   ...\n",
              "312  2025-01-12      Westward Ho!    167.316558  20.0\n",
              "313  2025-01-12      Whitsand Bay    129.146977  48.0\n",
              "314  2025-01-12     Widemouth Bay    158.416761  26.0\n",
              "315  2025-01-12  Wiseman's Bridge    111.968685  82.0\n",
              "316  2025-01-12    Woolacombe Bay    171.225299  15.0\n",
              "\n",
              "[317 rows x 4 columns]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "aggregated_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
